---
title: "Anomaly Detection"
author: "Kenny Darrell & Gerhard Pilcher"
date: "September 30, 2014"
output: html_document
---
# Predictive Analytics Times Article 


```{r echo = F}
## Outline

# 1.  What is an anomaly
#   * deviation from the common rule :  irregularity
#   *  something anomalous :  something different, abnormal, peculiar, or not easily classified
# 2.  Why should you care
# 3.  How they help generate questions
#   * Most statistics forces you to setup a hypothesis before you start
#   * Do I look for the best
#   * The worst 
#   * How many kinds of bad are there
# 4.  What is anomaly detection (outlier detection)
# 5.  How they help counteract human limitations
#   * mahalanobis gets around correlation
#   * cade helps with not know what to label things or are there even labels
```







### What is an Anomaly?
It is a hard question to answer. It is one of the things that you think you know it when you see it but quantification is a little more vague. Merriam-Webster claims that it is a deviation from the common rule, it is an irregularity. They say something anomalous, something different, abnormal, peculiar, or not easily classified. So by its very definition it citing an explicit case is hard because it means you can in some way classify it, or maybe the definition defines them in what they are not, I can’t classify it so it is an anomaly.

If an anomaly is something that is hard to classify, why is knowing about them relavent? In most cases this inability to classify them stems from there being abnormal or unwilling to follow the norm. Thus they are the extremities. For predictive analytics this is a really useful thing. The heart of predictive analytics is using models to to predict or classify what something is given some attributes related to thing in question. How to we determine what to classify though. Models that have made it the point of being able to predict anything with some level of confidence most likely have some root in anomaly detection. At some point a data scientist was given the task of building the model. There was with little doubt some question where to start. Knowing what the anomalies are is great starting point because it starts to put up some constraints around your problem. You start with everything, but as soon as you can identify some anomalies, you have some box around your known universe. The problem is a little more tangible.


### Why Should You Care?

Why should you care about the anomalies? Before the model is built or you can make any predictions you can learn a lot about the environment. By definition they are the things you did not really know about. They get you to ask new questions that you never knew you had. They start to break down the idioms that may have been created and force you to build a new set based on facts and data. 

Issues like best customer, mis-labeled data, drains on cash. Everyone has made a claim at some point in time based on an average, it is so easy to compute, and so easy to be contaminated by an anomaly. Let’s say you collect the age of people who come to your website. You need to make a decision about how to market to your user base and some smart marketing people have given you a few options based on your age demographic. Your real population is distributed as follows.

```{r, fig.align='center'}
pop <- rnbinom(n = 10000, size = 20, prob = .55)
hist(pop, main = 'Age Distribution')
```

Only it gets contaiminated by an error.
```{r}
cpop <- c(pop, 99999)
```



You would make the claim that your average user is `r mean(cpop)` years old. What does this mean. It means you go the route of marketing to people who are a few years out of college, maybe starting to get married. If you first did some anomaly detection and discoverd this observation of 99999 years old you would without a doubt remove it. You would then have an average age of `r mean(pop)`. These are very different people and very different points in there life. 



### How They Can Help

Knowing that some of your data has been corrupted turns out to be extremlye valuable. 'An outlier is a legitamite data point that exists far away from the other points. It may be required to remove these points for analytical purposes so they need to be found, but they are still valid. An anomaly is a point that was generated by another source altogether, the point is not even a legitamite value.' [ref](http://setandbma.wordpress.com/2014/05/08/anomalies-and-outliers/) Thus they need to be found as well but for differnt reasons. What other processes can inject values into our data. There must be some error that generated the 99999 value. This is actaully a common code given to missing values in older systems, so you will see things like this and probably worse if you work with data often. 



Finding anomalies like this helps us pose questions we never knew we had. They can help us find structural problems with our data.


How many of the problems being solved are moving forward based on indicators that are not true. So at the same time real data points can create anomalies to confine your universe, corrupt values can create anomalies to give you poor knowledge of the problem. Many areas have meaningful anomalies. Many problems can ignore the entirety of the data and can focus just on the anomalies, in fraud the only thing of interest is the anomaly, no need to classify or predict anything in the largest part of the population.

### What is Anomaly Detection

Thus analytics and those practicing should be concerned with finding anomalies. For this we require anomaly detection. How do we classify something that is hard to classify. It really boils down to finding those rare cases. A lot has been done to quantify and solve this problem, it is not a perfect science but you are not out of luck either. Great headway has been made in this area. Many techniques have been created that we can fall back on.

These methods, dubbed anomaly detection resemble much of the rest of analytics, stemming from drastically disparate domains for different use cases but come together to solve one cohesive sets of problems. They attemt to

From Wikipedia
In data mining, anomaly detection (or outlier detection) is the identification of items, events or observations which do not conform to an expected pattern or other items in a dataset.[1] Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or finding errors in text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions.[2]


Anomaly Detection helps counteract some of our human limiations.


```{r echo = F}
r <- .6     

x <- rnorm(1000,25,4)
y <- x * r + rnorm(1000, 0, 2) * sqrt(1 - r^2)


a <- 32
b <- 11

d <- data.frame(x, y)
d <- rbind(d, c(a, b))
```

```{r, echo = F, fig.align='center'}
hist(x, main = 'Would a point here be an Anomaly?')
abline(v=a,col="red")
```


```{r, echo = F, fig.align='center'}
hist(y, main = 'Would a point here be an Anomaly?')
abline(v=b,col="red")
```

What if the data has a correlation of .6.

```{r, echo = F, fig.align='center'}
plot(x, y, main = 'What about Now!')
points(x = a, y = b, col = "red")
```


If we find it hard to work with data that is correlated in only two diminsions how would we fare if it was in ten dimensions or even 100.



```{r echo = F}
mahalOutlier <- function(dataset, prop = .975) {
  # TODO: mahalOutlier: Add better check on numeric fields. They could be id, or one/zero.
  # Pull out numeric variables in data frame.
  nums <- sapply(dataset, is.numeric)
 
  # Create new dataframe with only these numeric varaibles.
  data <- dataset[, nums]
  
  # Calculate the mahalanobis distance for each entry.
  dataset$dist <- mahalanobis(data, colMeans(data), cov(data))
  
  # This is the cutoff distance to be considered an outlier.
  dataset$cut <- sqrt(qchisq(prop, ncol(data) - 1))
  
  # Generate the ids for the outliers.
  id <- seq(1, nrow(data))[dataset$dist >  dataset$cut]

  # Add one or zero to denote outliers.
  dataset$outlier <- 0
  dataset$outlier[id] <- 1
  
  dataset <- dataset[rev(order(dataset$dist)), 1:3]
  rownames(dataset) <- NULL
  
  return(dataset)
}


suppressWarnings({
library(VennDiagram, warn.conflicts = FALSE, quietly = TRUE)
library(xtable, warn.conflicts = FALSE, quietly = TRUE)
library(pander, warn.conflicts = FALSE, quietly = TRUE)
})
```



This approach comes from [Rick](http://blogs.sas.com/content/iml/2012/03/23/the-curse-of-dimensionality/) [Wicklin](http://blogs.sas.com/content/iml/2012/02/15/what-is-mahalanobis-distance/).




```{r eval = F}
mahalOutlier(d, .995)[1:5, ]
```

```{r kable, results="asis", echo = FALSE}

m <- mahalOutlier(d, .995)[1:5, ]
#print(xtable(m), type="html", include.rownames = getOption("xtable.include.rownames", FALSE), size = '      ')

panderOptions("table.split.table", Inf) 
pander(m)
```

We can see that our corrupted value has a distance from the center of `r m[1, ]$dist / m[1, ]$cut`  while the next closest furthest is `r m[2, ]$dist / m[2, ]$cut`, and they do not have many jumps.


#### Mahalanobis Distance

This method takes adavantage of the Mahalanobis Distance. The Mahalanobis Distance is a descriptive statistic that provides a relative measure of a data points distance from a central location (wiki). It calulates distances along the principal componenets. It takes into account the variance and covariances, which reduces to Euclidean distance for uncorrelated variables with unit variance. This has the great feature of bing unitless and scale-invariant, so we can compare across data sets. This method really shines in this case. It does have some assumptions of the data which require some preprocessing.

What about when you don't even know where to start. You have a bunch of data and want to know where the anomolies are. Mahalanobis works great in cases with numeric data that has normalish distributions. What if your data is more realistic and exhibits more exotic types of distributions among its varaibles, and they may not even numeric. There are other methods that will be more suited to this problem. 


#### CADE

Dr. David Jenson, along with Lisa Friedland and Amanda Gentzel have devised a method [CADE](http://people.cs.umass.edu/~lfriedl/pubs/SDM2014-paper.pdf), [Classifier Adjusted Density Estimation](http://people.cs.umass.edu/~lfriedl/pubs/SDM2014-supp.pdf) that provides some machinery to to help with this. What this method does is creates a new random data set that looks similar to the original except every varaible has a uniform distribution of the posible outcomes. We then give these observations new varaible, *anomaly*, with a value of 1. We give the real data the same variable but with a value of 0. We then run a classification algorithm on the combined sets of real and made up data. We train it to predict the *anomaly* varaible. We then through out all of the fake data and evaluate the model on the data we started with. The predicted probability of *anomaly* variable is now a measure of probablity of actaully being an outlier. 

#### Local Outlier Factor

Local Outlier Factor is an algorithm for identifying density-based local outliers. [ref](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.8948) An observation is classified as an outlier, relative to its neighbors, if its local density is significantly smaller than its neighbors' local densities. A method to measure distance must be decided and then it uses a K Nearest Neighbor approach. It uses these neighbors to determine the reachibilty distance, a density is calcu;ated based on all points that fall with this distance. Each points density is thn compared to those around. This procedure often requires a fair deal of cleaning to make the distance measures have some validity, it also require numeric type data were distances can be calculated at all.



#### Box and Whiskers


Perhaps the oldest method, the box plot, was created by John W. Tukey. This is more of agraphical way of seeing which observatins may be considered outliers.



#### K-Means

The k-means algorithm partitions the data into k groups by assigning them to the closest cluster centers. We then calculate the distance between each observation and its cluster center, and pick those with the largest distances as outliers. [ref](http://cran.r-project.org/doc/contrib/Zhao_R_and_data_mining.pdf#page=76)


### What does all of this mean?

One interesting place to test these methods out is see how they stack up against each other. A fun problem that anyone can relate to is the NBA. Some players are better than others. Some are so far ahead of the pack that they get labeled as an All Star. We can use each of the methods to try to pick out players who are outliers or anonlies from there in-game statistics. 

Each method was ran on the 2013 NBA regular season. Each method required its own types of cleansing and preprocessing. In the end some methods are easier to use than others simply due to how much up front preperation it takes to get the data ready. Not all data could be used in each case either, some methods have pretty strict requirements. Data was used where it could, it would not seem fair to limit the data fields some becuase methods could make use of them, that is just another part of there strengths and weaknesses.




```{r echo = F}
cade <- c('kevin durant', 'james harden', 'lebron james', 'carmelo anthony',
          'dwight howard', 'kobe bryant',  'paul george', 'russell westbrook',
          'joakim noah', 'tim duncan', 'dwyane wade', 'chris paul', 
          'brook lopez', 'lamarcus aldridge', 'kyrie irving', 'luol deng', 
          'zach randolph', 'jrue holiday', 'rajon rondo', 'blake griffin', 
          'chris bosh', 'tyson chandler', 'david lee', 'kevin garnett', 
          'tony parker')
```


```{r echo = F}
mahal <- c('lebron james', 'Stephen Curry', 'James Harden', 'Thabo Sefolosha', 
           'Carlos Boozer', 'Chris Paul', 'DeMar DeRozan', 'Greg Monroe',
           'Jimmy Butler', 'Kevin Durant', 'Kobe Bryant', 'O.J. Mayo', 
           'Paul Pierce', 'Ty Lawson', 'Blake Griffin', 'Brook Lopez', 
           'Byron Mullens', 'Deron Williams', 'Dwyane Wade', 'Jared Dudley',
           'John Salmons', 'Klay Thompson', 'LaMarcus Aldridge', 'Luol Deng',
           'Marco Belinelli', 'Maurice Harkless', 'Paul George',
           'Russell Westbrook', 'Al Horford', 'Corey Brewer')
```



```{r echo = F}
lof <- c('Ray Allen', 'Danny Green', 'Norris Cole', 'Mario Chalmers',
         'Tiago Splitter', 'Tyler Hansbrough', 'LeBron James', 'Ian Mahinmi',
         'Paul George', 'Roy Hibbert', 'Chris Bosh', 'Lance Stephenson',
         'Tayshaun Prince', 'Udonis Haslem', 'D.J. Augustin', 'Marc Gasol',
         'Mike Conley', 'George Hill', 'Jerryd Bayless', 'Jimmy Butler',
         'Klay Thompson', 'Nate Robinson', 'Shane Battier', 'Tony Allen',
         'Carl Landry', 'Harrison Barnes', 'David West', 'Kevin Durant',
         'Nick Collison', 'Thabo Sefolosha')
```



```{r echo = F}
box <- c('Dwyane Wade', 'LeBron James', 'Kevin Durant', 'Blake Griffin',
         'Chris Paul', 'Greg Monroe', 'James Harden', 'Paul Pierce',
         'Russell Westbrook', 'John Salmons', 'Kawhi Leonard', 'Mario Chalmers',
         'Paul George', 'Stephen Curry', 'Andre Iguodala', 'Brook Lopez',
         'Carmelo Anthony', 'Chandler Parsons', 'Danny Green', 'Serge Ibaka',
         'Thabo Sefolosha', 'Tony Parker', 'Brandon Knight', 'David Lee',  
         'DeAndre Jordan', 'Deron Williams', 'Dwight Howard', 'Ersan Ilyasova', 
         'George Hill', 'Gerald Wallace')

```


```{r echo = F}
km <- c('Kevin Durant', 'Jimmy Butler', 'Luol Deng', 'LeBron James',        
        'Carmelo Anthony', 'Kent Bazemore', 'Paul George', 'Brandon Jennings',   
        'Stephen Curry', 'Carlos Boozer', 'Chandler Parsons', 'DeMar DeRozan',
        'Joakim Noah', 'Klay Thompson', 'Kobe Bryant', 'Marco Belinelli',
        'Mike Conley', 'Nicolas Batum', 'Gerald Wallace', 'James Harden',
        'Jeff Green', 'Joe Johnson', 'Josh Smith', 'Kawhi Leonard', 
        'Monta Ellis', 'Nate Robinson', 'Paul Pierce', 'Tobias Harris',
        'Tyshawn Taylor', 'Al Horford')
```




```{r echo = F}

a <- tolower(cade)
b <- tolower(mahal)
c <- tolower(lof)
d <- tolower(km)
e <- tolower(box)


and <- function(num, ...) sum(table(c(...)) >= num)
  


x <- draw.quintuple.venn(
  area1 = length(a), area2 = length(b), area3 = length(c),
  area4 = length(d), area5 = length(e),
  n12 = and(2, a, b), n13 = and(2, a, c), n14 = and(2, a, d), n15 = and(2, a, e),
  n23 = and(2, b, c), n24 = and(2, b, d), n25 = and(2, b, e),
  n34 = and(2, c, d), n35 = and(2, c, e),
  n45 = and(2, d, e),
  n123 = and(3, a, b, c), n124 = and(3, a, b, d), n125 = and(3, a, b, e),
  n134 = and(3, a, c, d), n135 = and(3, a, c, e), n145 = and(3, a, d, e),
  n234 = and(3, b, c, d), n235 = and(3, b, c, e), n245 = and(3, b, d, e),
  n345 = and(3, c, d, e),
  n1234 = and(4, a, b, c, d), n1235 = and(4, a, b, c, e),
  n1245 = and(4, a, b, d, e), n1345 = and(4, a, c, d, e),
  n2345 = and(4, b, c, d, e),
  n12345 = and(5, a, b, c, d, e),
  category = c("CADE", "Mahalanobis", "Local Outlier Factor", 
               "K-Means", "Boxplot"),
  fill = c("dodgerblue", "goldenrod1", "darkorange1", "seagreen3", "orchid3"),
  cat.col = c("dodgerblue", "goldenrod1", "darkorange1", "seagreen3", "orchid3"),
  cat.cex = 2,
  margin = 0.05,
  cat.pos = c(0, 340, 180, 170, 10),
  cat.dist = c(.2, .22, .2, .2, .2),
  cat.diss = c(3, 3, 1, 3, 4),
  cex = c(1.5, 1.5, 1.5, 1.5, 1.5, 1, 0.8, 1, 0.8, 1, 0.8, 1, 0.8, 1, 0.8, 
          1, 0.55, 1, 0.55, 1, 0.55, 1, 0.55, 1, 0.55, 1, 1, 1, 1, 1, 1.5),
  ind = T
)


```


In the end it appears that there is a fair amount of overlap. Given that there are 100s of players in the NBA and there were only about 50 players tagged as an anomaly they are in some level of aggreement. In know way is it unanimous, but there each method shares some overlap. Those that are spotted by multi-methods are clearly very odd but each method brings something by finding some that were no found by any other method. It is not as clean as some of the ensemble methods of classification in that some form of voting would not work. It may be to pass on true anomalies to hedge against false positives. In most cases that anomoly detection is needed though this is not a problem. Simply rank them by how many methods they apeared. 


