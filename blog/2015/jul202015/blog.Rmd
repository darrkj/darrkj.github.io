---
title: "Untitled"
author: "Kenny Darrell"
date: "April 29, 2015"
output: html_document
---

A few months back I attended the [DC Ppen Data Day](http://dc.opendataday.org/). It was a pretty interesting event and hackathon. There were a lot of interesting problems. You can get an overview of some of the stuff we worked on [here](https://opendatadaydc.hackpad.com/GTFS-Data-Group-HeQOJPb0nm3).

I want to walk through some of this to address a common problem in data science. Here are some of the initializations you will need to follow along.

```{r init}
library(rvest)
library(dplyr)
library(leaflet)
library(igraph)
library(VennDiagram)

# This makes reading data in from text files much more logical.
options(stringsAsFactors = FALSE)
```

To get the data you can go to the [GTFS](http://www.gtfs-data-exchange.com/) website. There are many locations where this data is collected from. I have created a pretty simple way to get some of it.   

To do this we first create functions to crawl the site and find the locations of all of the sources zip files.

```{r download, eval = F}

# This function gets all of the URLs on the main page.
# The link to each location where data is contained.
get_all_urls <- function() {
  'http://www.gtfs-data-exchange.com/agencies' %>% 
    html() %>% html_nodes('a') %>% 
    html_attr('href') %>% 
    grep('agency', ., value = T) %>% 
    paste0('http://www.gtfs-data-exchange.com', .) -> locations
  
  # Last one is check
  locations[-length(locations)]
}

# This function will get the url for the zip file.
. %>% html() %>% 
  html_nodes('a') %>% 
  html_attr('href') %>%
  grep('.zip', ., value = T) %>% 
  grep('agency', ., value = T) %>%
  paste0('http://www.gtfs-data-exchange.com', .) -> get_zip_url
```


Now we can get a list of all of the cities. This will take a minute so we can just get a few of them. We also need a function to go to each site to download the the actual zip file. 

```{r eval = F}
all_zips <- lapply(get_all_urls()[1:3], get_zip_url)

# Function to download a zip file, unzip it and move text files around.
download_gtfs <- function(url, loc) {
  download.file(url = url, destfile = paste(loc, basename(url), sep = '/'))
  
  name <- gsub('http://www.gtfs-data-exchange.com/agency', loc, url)
  name <- gsub('/latest.zip', '', name)
  # Unzip downloaded file
  url %>% basename %>% paste(loc, ., sep = '/') %>% unzip(., exdir = name)
  unlink(paste(loc, basename(url), sep = '/'))
}
```


To use this we need to create a folder to store the data. For speed and space reasons lets just grab one of the locations data.

```{r eval = FALSE}
dir.create('gtfs_data')
download_gtfs(all_zips[[1]], 'gtfs_data/')
```

Once we have a folder of data from one location what do we do. This is often a problem whether you get acces to a database, and excel file with many worksheets or as we have here a folder with a collection of flat files. How do we figure out the structure of this data. This took some time each of us to understand how these tables were put together and how they were related. The issue I want to tackle here is the relations.

We can run this [code](https://github.com/darrkj/gtfs/blob/master/schema.R) to help with this.

```{r schema, echo = FALSE}
combs <- function(x) {
  # Initialize the returned object to null.
  ret <- NULL
  # There is no step needed for x as all others below 
  # x will touch it in there step.
  for(i in seq(x - 1)) {
    # Add the results from expand grid to the growing index list.
    ret <- rbind(ret, expand.grid(i, (i + 1):x))
  }
  return(ret)
}

Learn.Schema <- function(tableList, ignore = c()) {
  name <- lapply(tableList, names)
  len <- length(name)
  cmb <- combs(len)
  schema <- data.frame(x = character(), y = character(), z = character())
  net <- graph.empty() + vertices(names(tableList))
  for(i in 1:nrow(cmb)) {
    a <- cmb[i, 1]
    b <- cmb[i, 2]
    if(length(setdiff(intersect(name[[a]], name[[b]]), ignore)) > 0) {
      schema <- rbind(schema, 
              data.frame(x = names(tableList)[a], 
                         y = names(tableList)[b],
                         z = setdiff(intersect(name[[a]], name[[b]]), ignore)))
      net <- net + 
        edge(names(tableList)[a], names(tableList)[b],
             col = setdiff(intersect(name[[a]], name[[b]]), ignore))
    }
  }
  list(schema, net)
}

Get.Files <- function(dir = '.', delim = "|", pat = "", n = -1) {
  # This is all of the files, potentially with a specific patter.
  if (pat != "") {
    files <- grep(pat, list.files(dir), value = TRUE)
  } else {
    files <- list.files(dir)
  }
  # Join the file name to the dir.
  newFiles <- paste(dir, files, sep = "/")
  # Initialize arrays for names of fields.
  tables <- NULL
  # Loop over these files and append to a growing list of names.
  for (i in seq(newFiles)) {
    tables[[i]] <- read.csv(newFiles[i], sep = delim, header = TRUE, nrows = n)
  }
  names(tables) <- files
  return(tables)
}


Not.Connected <- function(schema) {
  notCon <- degree(schema)
  list(delete.vertices(schema, names(notCon[notCon == 0])),
       names(notCon[notCon == 0]))
}

Drop.Extremes <- function(schema) {
  Not.Connected(schema)[[1]]
}
```


The first thing we do is use the `Get.Files` function pointed at the folder with text files. This actually reads the data into a list of data.frames. Then we can run the `Learn.Schema` function on the list of data. 

```{r relate}
gtfs_data <- Get.Files('gtfs_data/a-reich-gmbh-busbetrieb/', delim = ',')

schema <- Learn.Schema(gtfs_data)

plot(schema[[2]])
```


We have one file that is really not realted to the rest. We can confirm what we have seen and drop it using `Not.Connected` and `Drop.Extremes`.

```{r}
Not.Connected(schema[[2]])

schema[[2]] <- Drop.Extremes(schema[[2]])
plot(schema[[2]])

```

This is now pretty useful, we can see the larger organization of this data, mainly how we could join one table onto another. We can see what fields we would join on by what they have in common.

```{r}
d <- schema[[1]]
d


d$a <- NA
d$b <- NA
d$c <- NA
 
 
for (i in 1:nrow(d)) {
  a <- gtfs_data[[d[i, 1]]][, d[i, 3]]
  b <- gtfs_data[[d[i, 2]]][, d[i, 3]]
 
  d$a[i] <- length(unique(a))
  d$b[i] <- length(unique(b))
  d$c[i] <- length(intersect(a, b))
}

d
```



```{r}
field <- 'agency_id'
x <- d[d[, 'z'] == field, ]
  
x <- unique(c(x$x, x$y))

a <- unique(gtfs_data[[x[1]]][, field])
b <- unique(gtfs_data[[x[2]]][, field])
ab <- length(intersect(a, b))
```


```{r echo = FALSE}
frame()
```

```{r eval = FALSE}
draw.pairwise.venn(length(a), length(b), length(intersect(a, b)), 
                   category = c(x[1], x[2]),
                   fill = c("goldenrod1", "darkorange1"),
                   cat.col = c("goldenrod1", "darkorange1"), cat.cex = 2)
```

Here we can see that one is completely contained inside the other.

```{r plot, eval = FALSE}
# Read the files into R.
routes     <- read.csv("gtfs_data/a-reich-gmbh-busbetrieb/routes.txt")
stop_times <- read.csv("gtfs_data/a-reich-gmbh-busbetrieb/stop_times.txt")
stops      <- read.csv("gtfs_data/a-reich-gmbh-busbetrieb/stops.txt")
trips      <- read.csv("gtfs_data/a-reich-gmbh-busbetrieb/trips.txt")


# Join the data together.
stops %>% 
  inner_join(stop_times, by = "stop_id") %>%
  inner_join(trips, by = "trip_id") %>%
  inner_join(routes, by = "route_id") %>%
  select(stop_id, trip_id, route_id, stop_lon, stop_lat) -> data


data[data$trip_id == data$trip_id[1], ] %>% arrange(route_id) -> ro1


(leaflet() %>% addTiles() %>%
   setView(lng = ro1$stop_lon[1], lat = ro1$stop_lat[1], zoom = 10) %>%
   addCircles(color = 'black', lat = ro1$stop_lat, lng = ro1$stop_lon))

```


