<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>Neo4j Soc Bal</title>



    <link rel="stylesheet" type="text/css" href="../../../stylesheets/blog.css">
<!-- Styles for R syntax highlighter -->
    <link rel="stylesheet" type="text/css" href="../../../stylesheets/blog2.css">
    <link rel="stylesheet" type="text/css" href="../../../stylesheets/stylesheet.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="../../../stylesheets/pygment_trac.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="../../../stylesheets/print.css" media="print" />
<!-- R syntax highlighter -->
    <script src="../../../javascripts/r.js"></script>


</head>

<body>

<div class="container">


    <header>
      <div class="container">
        <a href="http://darrkj.github.io/blogs" class="btn">Blogs</a>
        <h1>Simplifying Things with Neo4j</h1>
        <h2></h2>

<h4 class="author"><em>Kenny Darrell</em></h4>
<h4 class="date"><em>July 4, 2014</em></h4>
        </section>
      </div>
    </header>






<p>This material is from a month of lectures I used at Georgetown University for a graduate statistical programming course. I have read through plenty of tutorials and many have fallen out of date or been for setups that were quite different than mine. I am sure that since Spark has had a few new versions since most this was written that it will not work perfectly either. This will hopefully serve two purposes, point to sources that have some useful info on Spark and a place to store some of the useful commands I needed to get Spark to do real work. Spark and AWS are not the easyist peices of tech to work with. I have tried to work through many tutorials and most of them have an issue somewhere, this will not succeed where others have failed though.</p>
<div id="motivating-example" class="section level3">
<h3>Motivating Example</h3>
<p>To get started, a motivating example. <a href="http://stat-computing.org/dataexpo/2009/the-data.html">Flight data</a> from roughly 20 years.</p>
<pre class="r"><code>options(stringsAsFactors = FALSE)
library(dplyr)</code></pre>
<pre><code>## 
## Attaching package: 'dplyr'</code></pre>
<pre><code>## The following objects are masked from 'package:stats':
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from 'package:base':
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>library(lubridate)

system.time(f &lt;- read.csv('2008.csv', nrows = 100))</code></pre>
<pre><code>##    user  system elapsed 
##   0.004   0.000   0.004</code></pre>
<pre class="r"><code># How big is this 
format(object.size(f), units = &quot;auto&quot;)</code></pre>
<pre><code>## [1] &quot;22.2 Kb&quot;</code></pre>
<p>This dataset is pretty large but we can still work with the data fairly quickly once it is loaded into RAM.</p>
<pre class="r"><code>system.time(f %&gt;% group_by(Origin) %&gt;%
  summarize(incoming = n()) %&gt;%
  rename(airport = Origin) -&gt; inbound) </code></pre>
<pre><code>##    user  system elapsed 
##   0.008   0.001   0.009</code></pre>
<pre class="r"><code>system.time(f %&gt;% group_by(Dest) %&gt;%
  summarize(outgoing = n()) %&gt;%
  rename(airport = Dest) %&gt;%
  inner_join(inbound, by = 'airport') %&gt;%
  mutate(delta = incoming - outgoing) %&gt;%
  mutate(diff = abs(delta)) %&gt;%
  arrange(desc(diff)) -&gt; flights)</code></pre>
<pre><code>##    user  system elapsed 
##   0.004   0.000   0.005</code></pre>
<p>Once we get the data loaded into RAM working with it is not that bad, but the challange is getting into RAM.</p>
<p>I have never been able to get this plot to run. It is just to many observations for my machine!</p>
<pre class="r"><code>plot(f$DepTime, f$ArrTime)</code></pre>
<p>This plot will take forever and most likely crash!</p>
<p>A few notes so far:</p>
<ul>
<li>One year of data is roughly one gigabyte of RAM</li>
<li>We can do some analysis on this data</li>
<li>It will be very slow to make plots</li>
</ul>
<p>So what if we wanted to scale this up to all 22 years or more? It may work on some machines, but they would need to be powerful! What about if we had 100 years of data? What if we have more info on every flight? Having a more powerful machine is not that compelling of an option!</p>
<p>We could load all year like this. For most people this wont work, your machine will just bog down as it asypmtotically fills your RAM.</p>
<pre class="r"><code>f &lt;- list()
data &lt;- list.files(pattern = 'csv')[1:22]
for (i in data) {
  f[[i]] &lt;- read.csv(i)
  print(i)
  print(format(object.size(f), units = &quot;auto&quot;))
}</code></pre>
<p>What are our options? Is this where sampling comes into play? What if our goal was to find the page rank of each airport? Sampling works well in some cases and it others it is not valid. We can’t just sample a network, we could sample and get the average flights for day but not the pagerank!</p>
<ul>
<li><p>Getting a bigger more powerful machine has been the historic approach</p></li>
<li><p>Companies have bought bought bigger more powerful servers for years, but the are expensive.</p></li>
</ul>
<p>Will this always work?</p>
<ul>
<li><p>What if data collection continues to grow and next year this server fails?</p></li>
<li><p>I have to buy another one, then another. It does not scale.</p></li>
<li><p>If this is too expensive to get a bigger more powerful machine are there other options?</p></li>
</ul>
</div>
<div id="advanced-computing" class="section level2">
<h2>Advanced Computing</h2>
<p>There are three different methods which I would consider advanced computing.</p>
<ol style="list-style-type: decimal">
<li>Parallel Computing</li>
<li>Concurrent Computing</li>
<li>Distributed Computing</li>
</ol>
<div id="parallel-computing" class="section level3">
<h3>Parallel Computing</h3>
<p>The focus here is on getting the answer as fast as possible using multiple processors, whether they are the same machine or not. These usually have shared memory.</p>
</div>
<div id="concurrent-computing" class="section level3">
<h3>Concurrent Computing</h3>
<p>The focus here is working on differnt aspects of a problem at the same time using multiple threads. (not locking the user out while it performs some work) More a matter of architecture of software that architecture of hardware.</p>
</div>
<div id="distributed-computing" class="section level3">
<h3>Distributed Computing</h3>
<p>The focus here is on solving much bigger problems by using many machines. These machines are solving a peice of the larger problem by communicating across links via messages. They do not share memory like the above options and they are by defualt concurrent.</p>
<p>Which one is right for us?</p>
<ul>
<li>We were trying to scale up by bringing in the data across other years</li>
<li>When we are trying to solve problems with bigger amounts of data we have a distrubuted computing problem - More cores or threads would not help</li>
<li>We need more machines!</li>
</ul>
</div>
</div>
<div id="apache-hadoop-to-the-rescue" class="section level2">
<h2>Apache Hadoop to the Rescue</h2>
<p>Instead of trying to explain Hadoop and where it came from I will leave that to the expert. In the <a href="https://www.youtube.com/watch?v=eo1PwSfCXTI">The Evolution of the Apache Hadoop Ecosystem</a> Doug Cutting explains where Hadoop came from. In <a href="https://www.youtube.com/watch?v=Ttu3ZQ58ovo">The Apache Hadoop Ecosystem</a> he describes why it came to be.</p>
<p>Problem Solved</p>
<ul>
<li>So we need distributed computing</li>
<li>We have Hadoop</li>
<li>Problem solved</li>
</ul>
<p>Not so fast</p>
<ul>
<li>The first iteration of ‘Big Data’ was confusing</li>
<li>Everybody spoke about Hadoop and how great it was</li>
<li>They were not wrong, it did make all sorts of things possible</li>
<li>But many didn’t quite understood what it was, and they still probably do not.</li>
</ul>
<p>To effectivly use Hadoop …</p>
<ul>
<li>An expert in Linux</li>
<li>Plenty of knowledge of ETL and data processing skills</li>
<li>Expert in Java, which is object oriented, but mapreduce is functional,</li>
<li>You need to understand hardware</li>
<li>Knowledge of distrubted systems.</li>
</ul>
<p>Who understands this and knows how to use data? Nobody, well very few at least.</p>
<p>The people who actually use data to solve problems have two large issues to overcome.</p>
<p>Problem 1 - You have to write mapreduce code.</p>

<div class="separator" style="clear: both; text-align: center;"><img src="java.png" height="550" width="700"></a></div>

<p>Problem 2 - You have to actually setup a cluster.</p>


<div class="separator" style="clear: both; text-align: center;"><img src="cluster.jpg" height="550" width="700"></a></div>

<p>Thus Hadoop may not be the solution!</p>
<ul>
<li>Hadoop is so 2012</li>
<li>Data Scientists should not be writing code for Hadoop</li>
<li>The ecosystem is changing very frequently to resolve this issue</li>
<li><a href="https://www.dataquest.io/mission/123/introduction-to-spark/">History</a></li>
</ul>
</div>
<div id="and-then-there-was-spark" class="section level2">
<h2>And then there was Spark</h2>


<div class="separator" style="clear: both; text-align: center;"><img src="spark-logo.png" height="550" width="700"></a></div>



<p>Remember the mapreduce code. This is what mapreduce code looks like</p>


<div class="separator" style="clear: both; text-align: center;"><img src="mapred.png" height="550" width="700"></a></div>



<p>This is the equivalent code in Spark</p>


<div class="separator" style="clear: both; text-align: center;"><img src="spark_word.png" height="550" width="700"></a></div>



<p>It is also much faster</p>


<div class="separator" style="clear: both; text-align: center;"><img src="spark_v_hadoop.png" height="550" width="700"></a></div>


<p>So Hadoop is old news. If that did not convince you maybe this will. This is a collection of job posting. Each blue line is a job that mentiones Hadoop while each red line mentions Spark.</p>


<div class="separator" style="clear: both; text-align: center;"><img src="spark_jobs.jpeg" height="550" width="700"></a></div>


<p>Today</p>
<ul>
<li><p>Spark came along and got rid of the need to be an expert Java programmer.</p></li>
<li><p>Thus we may be able to say we have solved problem number 1.</p></li>
</ul>
<p>Spark Ecosystem</p>


<div class="separator" style="clear: both; text-align: center;"><img src="ecosys.jpg" height="550" width="700"></a></div>


<p>This all makes Spark a hit</p>
<p>Spark is a hit for data science!</p>

<div class="separator" style="clear: both; text-align: center;"><img src="hit.png" height="550" width="700"></a></div>


<p>Using Spark</p>
<p>If you do some looking around you may find some of these options;</p>
<div id="build-spark" class="section level3">
<h3>Build Spark</h3>
<p>Getting setup with spark <a href="http://spark.apache.org/downloads.html">standalone</a>, is a little difficult, need to have Scala and SBT setup</p>
</div>
<div id="virtual-machine" class="section level3">
<h3>Virtual Machine</h3>
<ul>
<li><p>There is a <a href="http://thegrimmscientist.com/2014/12/01/vagrant-tutorial-spark-in-a-vm/">vagrant</a> option</p></li>
<li><p>Which requires <a href="https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/courseware/d1f293d0cb53466dbb5c0cd81f55b45b/920d3370060540c8b21d56f05c64bdda/">virtual box</a></p></li>
</ul>
</div>
<div id="docker" class="section level3">
<h3>Docker</h3>
<ul>
<li>Using <a href="https://sparktutorials.github.io/2015/04/14/getting-started-with-spark-and-docker.html">docker</a></li>
</ul>
</div>
<div id="aws" class="section level3">
<h3>AWS</h3>
<ul>
<li><p>Running Spark on <a href="https://aws.amazon.com/articles/Elastic-MapReduce/4926593393724923">aws</a></p></li>
<li><p>Using <a href="http://www.slideshare.net/felixcss/sparkly-notebook-interactive-analysis-and-visualization-with-spark">spark and jupyter</a>, streaming means,</p></li>
<li>Spark is a giant tradeoff</li>
<li>It is usable by data scientists</li>
<li>It is also usable by software engineers and devops folks</li>
<li>Non of the above options seem very appealing</li>
<li><p>There has to be a better way</p></li>
</ul>
<p>Using Spark</p>
<p>We are going to use spark in standalone or local mode</p>
<p>Instructions for Downloading Spark (versions will change)</p>
<p>In any browser navigate to the download <a href="http://spark.apache.org/downloads.html">page</a></p>
<ol style="list-style-type: decimal">
<li><p>For the release type select ‘1.5.1 (Oct 02 2015)’</p></li>
<li><p>For package type select ‘Pre-built for Hadoop 2.6 and later’</p></li>
<li><p>For download type select ‘Direct Download’</p></li>
<li><p>Click the link to download</p></li>
</ol>


<div class="separator" style="clear: both; text-align: center;"><img src="https://github.com/Math510/Spark-stuff/blob/master/download_screenshot.png" height="550" width="700"></a></div>


<p>Unzip the file</p>
<p>On windows use 7zip or</p>
<p>On Mac or Linux run the following in the directory containing the download</p>
<pre><code>tar zxvf spark-1.5.1-bin-hadoop2.6.tgz</code></pre>
<p>What is the difference?</p>
<ul>
<li>So far the options are more on the software engineering side</li>
<li>They let you use Spark in a cluster setting</li>
<li>Local mode is still constrained by a your machine</li>
<li>But it is much easier to get setup</li>
<li>It’s also the exact same</li>
</ul>
<pre><code>sparkR.init(master=&quot;local&quot;)</code></pre>
<p>vs</p>
<pre><code>sparkR.init(master=&quot;spark://&lt;master&gt;:7077&quot;)</code></pre>
<p>Spark API</p>
<ul>
<li>We can use sparkr or pyspark just like we would use r or python</li>
<li><a href="https://districtdatalabs.silvrback.com/getting-started-with-spark-in-python">Python option</a></li>
<li><a href="https://github.com/amplab-extras/SparkR-pkg/wiki/SparkR-Quick-Start">R option</a></li>
</ul>
<p>For data science</p>
<p><a href="http://www.stat.wvu.edu/~jharner/Interface2015Slides/LinTutorialSlides.pdf">slides on sparkr</a></p>
<p>Sparkr Command line</p>
<pre><code>./bin/sparkR</code></pre>
<pre class="r"><code>sc &lt;- sparkR.init(&quot;local&quot;)
lines &lt;- SparkR:::textFile(sc, '../README.md')
length(lines)</code></pre>
<p>Get spark <a href="http://www.r-bloggers.com/installing-and-starting-sparkr-locally-on-windows-os-and-rstudio/">running in rstudio</a>.</p>
<pre class="r"><code>Sys.setenv(SPARK_HOME = '/Users/kdarrell/Desktop/Spark-1.5.1')

Sys.getenv(&quot;SPARK_HOME&quot;)

.libPaths(c(file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;, &quot;lib&quot;), .libPaths()))

# Make sure you don't have spark from cran!
library(SparkR)

sc &lt;- sparkR.init(&quot;local&quot;)

lines &lt;- SparkR:::textFile(sc, 'week 2.Rmd')
length(lines)</code></pre>
<p>Turn log options to less verbose!</p>
<p>To get rid of this make a copy of this file:</p>
<pre><code>conf/log4j.properties.template</code></pre>
<pre><code>conf/log4j.properties</code></pre>
<p>And make the following change:</p>
<p>Replace this line</p>
<pre><code>log4j.rootCategory=INFO, console</code></pre>
<p>With this</p>
<pre><code>log4j.rootCategory=WARN, console</code></pre>
<p>What all can we do in Spark?</p>
<p>For access to data.frames we need Spark SQL</p>
<pre class="r"><code>sqlContext &lt;- sparkRSQL.init(sc)

df &lt;- createDataFrame(sqlContext, faithful) </code></pre>
<p>Is it really the same?</p>
<pre class="r"><code># This works
head(faithful)
head(df)

# And this
faithful$eruptions
df$eruptions

# But not this
faithful[1, ]
df[1, ]


lm(faithful$eruptions ~ faithful$waiting)
lm(df$eruptions ~ df$waiting)</code></pre>
<p>So many differences</p>
<p>We are not really using R any more</p>
<p>We are using and api to spark inside of R The api is growing to act more like R and support more of its functionality though</p>
<p>Data Science</p>
<p>So if we want to do machine learning stuff what do we do</p>
<pre class="r"><code># https://spark.apache.org/docs/latest/sparkr.html#creating-dataframes
# Create the DataFrame
df &lt;- createDataFrame(sqlContext, iris)

SparkR::glm
stats::glm

# Fit a linear model over the dataset.
model &lt;- glm(Sepal_Length ~ Sepal_Width + Species, data = df, family = &quot;gaussian&quot;)

summary(model)

predictions &lt;- predict(model, newData = df)
head(select(predictions, &quot;Sepal_Length&quot;, &quot;prediction&quot;))</code></pre>
<p>Back to the flight data</p>
<p>Can we now read the csv in from earlier</p>
<pre class="r"><code># First thought
df &lt;- createDataFrame(sqlContext, read.csv('../flight/2008.csv'))

# Second thought
df &lt;- read.df(sqlContext, '../flight/2008.csv', source = &quot;csv&quot;)

# Third thought
customSchema &lt;- structType(
    structField(&quot;year&quot;, &quot;integer&quot;), 
    structField(&quot;make&quot;, &quot;string&quot;),
    structField(&quot;model&quot;, &quot;string&quot;),
    structField(&quot;comment&quot;, &quot;string&quot;),
    structField(&quot;blank&quot;, &quot;string&quot;))

df &lt;- read.df(sqlContext, &quot;cars.csv&quot;, source = &quot;com.databricks.spark.csv&quot;, schema = customSchema)</code></pre>
<p>The issue</p>
<p>Need the <a href="https://github.com/databricks/spark-csv">csv pacakge</a></p>
<p>How?</p>
<pre><code>bin/spark-shell --packages com.databricks:spark-csv_2.10:1.0.3</code></pre>
<pre><code>sparkR --packages com.databricks:spark-csv_2.10:1.0.3</code></pre>
<p>Spark has packages too</p>
<p>Until next time!</p>
</div>
</div>
<div id="week-2" class="section level2">
<h2>week 2</h2>
</div>
<div id="running-spark" class="section level2">
<h2>Running Spark</h2>
<p>Java issues?</p>
<p>Thus we could run Spark on its own.</p>
<p>In the location where we have spark</p>
<p>On a Mac</p>
<pre><code>./bin/spark-shell</code></pre>
<p>On Windows (cross your fingers) Possible windows issues, may need to <a href="http://nishutayaltech.blogspot.com/2015/04/how-to-run-apache-spark-on-windows7-in.html">this</a>.</p>
<pre><code>bin\spark-shell</code></pre>
<p>We are now in the Scala repl. More on this later.</p>
</div>
<div id="review-from-last-week" class="section level2">
<h2>Review from last week</h2>
<p>Last week we got an intro to a few concepts that we should review.</p>
<ol style="list-style-type: decimal">
<li>Limitations of systems like R and Python</li>
<li>Distributed computing</li>
<li>Hadoop/HDFS</li>
<li>Spark</li>
<li>SparkR</li>
<li>API</li>
</ol>
</div>
<div id="limitations-of-systems-like-r-and-python" class="section level2">
<h2>Limitations of systems like R and Python</h2>
<p>R and Python are great for small and medium sized</p>
<p>There are many other (attempted) defintions of these, but for our purpose this one works pretty well. (Thanks to Hadley Wickham)</p>
<p>Small Data - Fits in memory on a laptop: &lt; 10 GB Medium Data - Fits in memory on a server or disk on a laptop: 10 GB - 1 TB Big Data - Can’t fit in memory on any one machine: &gt; 1 TB</p>
<p>R and Python are great for Small Data. They can be good for Medium data if you have a server or with some decent coding chops on a single laptop. They are both terrible at number Big Data, on there own that is.</p>
</div>
<div id="distributed-computing-1" class="section level2">
<h2>Distributed computing</h2>
<p>Distributed computing was made much easier (not easy just easier) via research by Google that eventually reached open source by the name Hadoop/HDFS.</p>
<p>Distributed computing is the method of using the RAM of many computers to attack a problem, so it is best equipped for volume problems as opposed to variety problems (concurrent computing) and speed constraints (parallel computing).</p>
</div>
<div id="hadoophdfs" class="section level2">
<h2>Hadoop/HDFS</h2>
<p>Hadoop and HDFS perform extremely poorly on Small Data. It can be okay on Medium Data. They often take much longer than the Python and R methods for very similar problems on small/medium data. For non-distrubted systems and software engineer folks they can be difficult to actaully get up and running.</p>
</div>
<div id="spark" class="section level2">
<h2>Spark</h2>
<p>Spark was an improvement over Hadoop in almost every sense. It can perform better on Small and Medium Data than Hadoop/HDFS. Not a clear winner when it comes to comparison of R and Python on these sizes though. It is also much easier to get up and running sense it supports various backends and can run in a local mode.</p>
</div>
<div id="sparkr" class="section level2">
<h2>SparkR</h2>
<p>This was made avalable first to Python users via pyspark and then to R users via SparkR.</p>
</div>
<div id="api" class="section level2">
<h2>API</h2>
<p>This avalability is via an API.</p>
<p>API - Application Programming Interface</p>
<p>An API is an interface that provides access to some functionality. You don’t have to worry about the underlying implementation, you can use the interface as a set of building blocks to solve some problem or create something.</p>
</div>
<div id="back-to-spark-actaully-scala" class="section level2">
<h2>Back to Spark, actaully Scala</h2>
<p>Scala is programming language that compiles to Java Bytecode and can run on the jvm the same as Java. It is about 10 years old. Many have claimed that it will replace Java in terms of new development. It makes many things easier.</p>
<p>Many people do not like it as they think it is hard to learn.</p>
<p>Have no fear, this is not a uniformly distributed statement.</p>
<p>If you have learned R you will have a pretty easy time picking up Scala.</p>
<p>Java Fibonacci</p>
<pre><code>public class Fibonacci {
    public static int fib(int n) {
                int prev1=0, prev2=1;
                for(int i=0; i&lt;n; i++) {
                    int savePrev1 = prev1;
                    prev1 = prev2;
                    prev2 = savePrev1 + prev2;
                }
                return prev1;
    }
}</code></pre>
<p>Scala Fibonacci</p>
<pre><code>def fib( n : Int) : Int = n match {
   case 0 | 1 =&gt; n
   case _ =&gt; fib( n-1 ) + fib( n-2 )
}</code></pre>
<p>R Fibonacci</p>
<pre class="r"><code>fib &lt;- function(n) {
  if (n %in% c(0, 2)) n
  else fib(n - 1) + fib(n - 2)
}</code></pre>
<p>The biggest thing to note is that in Scala you need to say either val or var before you create a variable.</p>
<pre><code>val textFile = sc.textFile(&quot;README.md&quot;)
textFile.count()
textFile.first()</code></pre>
<p>Some things to note about Spark</p>
<p>It is lazy, in the computing sense. This means that it does not compute anything until it actually needs the result for something. This is good and bad.</p>
<p>This is known as lazy evaluation.</p>
<p>Pros</p>
<ol style="list-style-type: decimal">
<li>Performance increases by avoiding needless calculations</li>
<li>The ability to construct potentially infinite data structures</li>
</ol>
<p>Cons</p>
<ol style="list-style-type: decimal">
<li>Profiling becomes hard, work may not be done when you expect it to be.</li>
</ol>
<p>The opposite is known as eager evaluation.</p>
<pre><code>val linesWithSpark = textFile.filter(line =&gt; line.contains(&quot;Spark&quot;))
textFile.filter(line =&gt; line.contains(&quot;Spark&quot;)).count()</code></pre>
<p>These hightlight the two types of operations that can happen to and RDD</p>
<ol style="list-style-type: decimal">
<li>Transformations - build a new RDD<br />
</li>
<li>Actions - compute and output results</li>
</ol>
<p>The basic building block of spark is the RDD (Resilient Distributed Dataset) RDD’s are an immutable, partitioned collection of elements that can be operated on in parallel.</p>
<p>Spark is pretty well <a href="http://spark.apache.org/docs/latest/">documented</a>.</p>
<p><a href="http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf">RDD paper</a></p>
</div>
<div id="doing-math-with-spark" class="section level2">
<h2>Doing math with spark</h2>
<pre><code>val count = sc.parallelize(1 to 10000000).map{i =&gt;
  val x = Math.random()
  val y = Math.random()
  if (x*x + y*y &lt; 1) 1 else 0
}
count.collect</code></pre>
<pre><code>./bin/spark-shell --packages com.databricks:spark-csv_2.10:1.2.0

import org.apache.spark.sql.SQLContext

val sqlContext = new SQLContext(sc)

val df = sqlContext.read.format(&quot;com.databricks.spark.csv&quot;).option(&quot;header&quot;, &quot;true&quot;).option(&quot;inferSchema&quot;, &quot;true&quot;).load(&quot;1996.csv&quot;)

val ori = df.groupBy(&quot;Origin&quot;).count().withColumnRenamed(&quot;count&quot;, &quot;in&quot;)
ori.show()

val des = df.groupBy(&quot;Dest&quot;).count().withColumnRenamed(&quot;count&quot;, &quot;out&quot;)
des.show()


ori.registerTempTable(&quot;ori&quot;)
des.registerTempTable(&quot;des&quot;)


val res = ori.join(des, ori(&quot;Origin&quot;) === des(&quot;Dest&quot;))

res.registerTempTable(&quot;res&quot;)

sqlContext.sql(&quot;SELECT Origin, in - out FROM res&quot;).show</code></pre>
</div>
<div id="using-the-apis" class="section level2">
<h2>Using the APIs</h2>
<p>Show pyspark here</p>
<pre><code>log = sc.textFile(&quot;README.md&quot;).cache()
spark = log.filter(lambda line: &quot;Spark&quot; in line)

</code></pre>
<p>SparkR</p>
<pre><code>./bin/sparkR</code></pre>
</div>
<div id="inside-of-rstudio" class="section level2">
<h2>Inside of rstudio</h2>
<pre class="r"><code># This is the location where spark exists
Sys.setenv(SPARK_HOME = '/Users/kdarrell/Desktop/Spark-1.5.1')

# Proof that the above code actually worked.
Sys.getenv(&quot;SPARK_HOME&quot;)

# Location of the valid SparkR package.
.libPaths(c(file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;, &quot;lib&quot;), .libPaths()))

# Make sure you don't have SparkR from cran!
library(SparkR)


# Starting Spark
sc &lt;- sparkR.init(&quot;local&quot;)
sparkR.stop()
# You can start it with a reference to packages.
Sys.setenv('SPARKR_SUBMIT_ARGS'='&quot;--packages&quot; &quot;com.databricks:spark-csv_2.10:1.2.0&quot; &quot;sparkr-shell&quot;')
sc &lt;- sparkR.init(&quot;local&quot;)
sparkR.stop()
# You can start it give it more memory.
sc &lt;- sparkR.init(&quot;local&quot;, sparkEnvir=list(spark.executor.memory=&quot;10g&quot;))


# Create a file.
cat(state.name, file = 'states.txt', sep = '\n')

# Read the file.
lines &lt;- SparkR:::textFile(sc, 'states.txt')
length(lines)

#sparkR.stop()</code></pre>
<p>using sql context for data frames</p>
<pre class="r"><code>sqlContext &lt;- sparkRSQL.init(sc)

df &lt;- createDataFrame(sqlContext, faithful) </code></pre>
<p>Is it really the same?</p>
<pre class="r"><code># This works
head(faithful)
head(df)

# And this
faithful$eruptions
df$eruptions

# But not this
faithful[1, ]
df[1, ]


lm(faithful$eruptions ~ faithful$waiting)
lm(df$eruptions ~ df$waiting)</code></pre>
<p>So if we want to do machine learning stuff what do we do</p>
<pre class="r"><code># https://spark.apache.org/docs/latest/sparkr.html#creating-dataframes
# Create the DataFrame
df &lt;- createDataFrame(sqlContext, iris)

SparkR::glm
stats::glm

# Fit a linear model over the dataset.
model &lt;- glm(Sepal_Length ~ Sepal_Width + Species, data = df, family = &quot;gaussian&quot;)

summary(model)

predictions &lt;- predict(model, newData = df)
head(select(predictions, &quot;Sepal_Length&quot;, &quot;prediction&quot;))</code></pre>
<p>Running SQL</p>
<pre class="r"><code>faith &lt;- createDataFrame(sqlContext, faithful)

# Register this DataFrame as a table.
registerTempTable(faith, &quot;faith&quot;)

# SQL statements can be run by using the sql method
long &lt;- sql(sqlContext, &quot;SELECT * FROM faith WHERE waiting &gt; 50&quot;)
head(long)</code></pre>
<pre class="r"><code>printSchema(df)
cache(df)
head(df)
columns(df)
count(df)</code></pre>
<p>should end this way</p>
<pre class="r"><code>sparkR.stop()</code></pre>
<p>Using the Spark Graphx API from Scala with the flight data.</p>
<pre><code>
./bin/spark-shell --packages com.databricks:spark-csv_2.10:1.2.0

import org.apache.spark.sql.SQLContext
import org.apache.spark.graphx._
import org.apache.spark.rdd.RDD
import scala.util.MurmurHash

val sqlContext = new SQLContext(sc)

val df = sqlContext.read.format(&quot;com.databricks.spark.csv&quot;).option(&quot;header&quot;, &quot;true&quot;).option(&quot;inferSchema&quot;, &quot;true&quot;).load(&quot;1996.csv&quot;)


val flightsFromTo = df.select($&quot;Origin&quot;,$&quot;Dest&quot;)
val airportCodes = df.select($&quot;Origin&quot;, $&quot;Dest&quot;).flatMap(x =&gt; Iterable(x(0).toString, x(1).toString))

val airportVertices: RDD[(VertexId, String)] = airportCodes.distinct().map(x =&gt; (MurmurHash.stringHash(x), x))
val defaultAirport = (&quot;Missing&quot;)

val flightEdges = flightsFromTo.map(x =&gt;
    ((MurmurHash.stringHash(x(0).toString),MurmurHash.stringHash(x(1).toString)), 1)).reduceByKey(_+_).map(x =&gt; Edge(x._1._1, x._1._2,x._2))
    
val graph = Graph(airportVertices, flightEdges, defaultAirport)
graph.persist() // we're going to be using it a lot


graph.numVertices // 213
graph.numEdges // 3189



graph.triplets.sortBy(_.attr, ascending=false).map(triplet =&gt;
    &quot;There were &quot; + triplet.attr.toString + &quot; flights from &quot; + triplet.srcAttr + &quot; to &quot; + triplet.dstAttr + &quot;.&quot;).take(10)

graph.triplets.sortBy(_.attr).map(triplet =&gt;
    &quot;There were &quot; + triplet.attr.toString + &quot; flights from &quot; + triplet.srcAttr + &quot; to &quot; + triplet.dstAttr + &quot;.&quot;).take(10)
    
    
    
    
graph.inDegrees.join(airportVertices).sortBy(_._2._1, ascending=false).take(1)

graph.outDegrees.join(airportVertices).sortBy(_._2._1, ascending=false).take(1)



val ranks = graph.pageRank(0.0001).vertices


val ranksAndAirports = ranks.join(airportVertices).sortBy(_._2._1, ascending=false).map(_._2._2)
ranksAndAirports.take(10)
</code></pre>
<pre class="r"><code>fl &lt;- read.csv('../flight/1996.csv')
library(igraph)</code></pre>
</div>
<div id="week-3" class="section level2">
<h2>week 3</h2>
</div>
<div id="getting-setup" class="section level2">
<h2>Getting setup</h2>
<p><a href="http://sparktutorials.net/spark-clusters-on-aws-ec2---reading-and-writing-s3-data---predicting-flight-delays-with-spark-part-1">Overall tutorial</a></p>
<p><a href="http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-plan-upload-s3.html?tag=vig-20">S3 tutorial</a></p>
<p><a href="http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-plan-upload-s3.html#create-s3-bucket-input">create s3 bucket</a></p>
<p><a href="http://docs.aws.amazon.com/AmazonS3/latest/gsg/PuttingAnObjectInABucket.html">putting stuff in a bucket</a></p>
</div>
<div id="the-second-hurdle" class="section level2">
<h2>The second Hurdle</h2>
<p>A few weeks back we two problems in taking advantage of distributed computing.</p>
<p>You have to write mapreduce code. Spark solves this problem.</p>
<p>You have to actually setup a cluster. AWS takes care of this problem.</p>
<p>AWS removes the need to have a physichal cluster.</p>
</div>
<div id="creating-accounts" class="section level2">
<h2>Creating Accounts</h2>
<ol style="list-style-type: decimal">
<li><p>Create an aws account - can create an EC2 instance, computer in the cloud</p></li>
<li><p>Get data in S3 buckets - your data also now resides in the cloud</p></li>
<li><p>Create the EMR cluster - multiple machines preconfigured for large data analysis (a collection of EC2 instances with Hadoop/Spark and other ‘big data’ stuff loaded)</p></li>
<li><p>Use Spark ec2 scripts - customized for using Spark to analyze data</p></li>
</ol>
<p>From bash</p>
<pre><code>export AWS_ACCESS_KEY_ID=&lt;YOURKEY&gt;
export AWS_SECRET_ACCESS_KEY=&lt;YOURSECRETKEY&gt;</code></pre>
<p>On Windows you need to add these as Environment Variables.</p>
<p>Starting cluster</p>
<pre><code>./spark-ec2 --key-pair=ken_key --identity-file=ken_key.pem --region=us-west-2 -s 15 -v 1.5.1 --copy-aws-credentials launch airplane-cluster</code></pre>
<p>Wait some amount of time for cluster to be constructed.</p>
<p>In reality you may want to create one and pause it. If you did not need Hadoop/HDFS this would be faster. Companies like Cloudera and HortonWorks can help here as well.</p>
<p>Where my data <a href="https://console.aws.amazon.com/s3/home?region=us-west-2#&amp;bucket=flightsdata&amp;prefix=">exists</a></p>
<p>Logging into the cluster</p>
<pre><code>./spark-ec2 -k ken_key -i ../ken_key.pem --region=us-west-2 login airplane-cluster</code></pre>
<p>Moniter the <a href="http://ec2-52-33-202-77.us-west-2.compute.amazonaws.com:8080/">cluster</a></p>
<p>Once again</p>
<pre><code>export AWS_ACCESS_KEY_ID=&lt;YOURKEY&gt;
export AWS_SECRET_ACCESS_KEY=&lt;YOURSECRETKEY&gt;</code></pre>
<p>Start spark on the cluster</p>
<pre><code>./spark/bin/spark-shell</code></pre>
<p>Now some Scala code, but we could have put R/Python here becuase every machine in our cluster has them installed.</p>
<pre><code>import org.apache.spark.sql.SQLContext
import sqlContext.implicits._
val sqlContext = new SQLContext(sc)

case class Flight(Origin: String, Dest: String)

val flight = sc.textFile(&quot;s3n://flightsdata/*&quot;).map(_.split(&quot;,&quot;)).map(x =&gt; x.slice(16, 18)).map(p =&gt; Flight(p(0), p(1))).toDF()


val ori = flight.groupBy(&quot;Origin&quot;).count().withColumnRenamed(&quot;count&quot;, &quot;in&quot;)
ori.show()

val des = flight.groupBy(&quot;Dest&quot;).count().withColumnRenamed(&quot;count&quot;, &quot;out&quot;)
des.show()


ori.registerTempTable(&quot;ori&quot;)
des.registerTempTable(&quot;des&quot;)

val res = ori.join(des, ori(&quot;Origin&quot;) === des(&quot;Dest&quot;))

res.registerTempTable(&quot;res&quot;)</code></pre>
<p>Destroy the cluster</p>
<pre><code>./spark-ec2 -k ken_key -i ../ken_key.pem --region=us-west-2 destroy airplane-cluster</code></pre>
<p>This cost almost nothing, what we really did was rent the the exact machine we needed. If we need a bigger one to scale with our data we just increment the number of machines.</p>
</div>


</div>




<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with --self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>

